"""
step 4999 loss: 0.10316227376461029
Input: ['MJTTYGPOGH'], Should get: HGOPGYTTJM, Model generate: HGOPGYTJTM  |The result is False
Input: ['ETRSMXSXNJ'], Should get: JNXSXMSRTE, Model generate: JNXSXMSRSW  |The result is False
Input: ['HQCTZAZYUP'], Should get: PUYZAZTCQH, Model generate: PUYZAZTQCH  |The result is False
Input: ['REFAFIMZLY'], Should get: YLZMIFAFER, Model generate: YLZMIFAFER  |The result is True
Input: ['EICYXVXDWN'], Should get: NWDXVXYCIE, Model generate: NWDXVXYCIE  |The result is True
Input: ['QUSKFDPQKZ'], Should get: ZKQPDFKSUQ, Model generate: ZKQPDFKSUQ  |The result is True
Input: ['UJJPPYXIUX'], Should get: XUIXYPPJJU, Model generate: XUPIYXJUXL  |The result is False
Input: ['PYMKUQZRVG'], Should get: GVRZQUKMYP, Model generate: GVRZQUKMYP  |The result is True
Input: ['OKPBKGRCBM'], Should get: MBCRGKBPKO, Model generate: MBCRGKBPKO  |The result is True
Input: ['RLPOMAZYOX'], Should get: XOYZAMOPLR, Model generate: XOYZAMOPLR  |The result is True
Input: ['NFCWCVGFQM'], Should get: MQFGVCWCFN, Model generate: MQFGVCWFCN  |The result is False
Input: ['JMPQTWECXZ'], Should get: ZXCEWTQPMJ, Model generate: ZXCEWTQPJM  |The result is False
Input: ['WFENYQCVPL'], Should get: LPVCQYNEFW, Model generate: LPVCQYNEFW  |The result is True
Input: ['PIOWMPVIKO'], Should get: OKIVPMWOIP, Model generate: OKIVMPWOPI  |The result is False
Input: ['WHVPBNADIG'], Should get: GIDANBPVHW, Model generate: GIDANBPHVW  |The result is False
Input: ['GCVTWNEGRK'], Should get: KRGENWTVCG, Model generate: KRGENWTVCG  |The result is True
Input: ['AQPOCQCQMB'], Should get: BMQCQCOPQA, Model generate: BMQCQCPOQA  |The result is False
Input: ['JVSTKVCWSG'], Should get: GSWCVKTSVJ, Model generate: GSWCVKTSVJ  |The result is True
Input: ['WXFMXMLSOY'], Should get: YOSLMXMFXW, Model generate: YOSLMXXFMW  |The result is False
Input: ['JZECOFYCQJ'], Should get: JQCYFOCEZJ, Model generate: JQCYFOCEZJ  |The result is True
Input: ['BUJIYZRWYY'], Should get: YYWRZYIJUB, Model generate: YYWRZYIJUB  |The result is True
Input: ['ULQMFRIATJ'], Should get: JTAIRFMQLU, Model generate: JTAIRFMQLU  |The result is True
Input: ['ZJBNZRVTPS'], Should get: SPTVRZNBJZ, Model generate: SPTVRZNBJZ  |The result is True
Input: ['XQLIWTIFEC'], Should get: CEFITWILQX, Model generate: CEFITWIQLX  |The result is False
Input: ['IFKWJXCGXD'], Should get: DXGCXJWKFI, Model generate: DXGCXJWKFI  |The result is True
Input: ['ZXMKNDRTOD'], Should get: DOTRDNKMXZ, Model generate: DOTRDNKMXZ  |The result is True
Input: ['GHWQMHCSKL'], Should get: LKSCHMQWHG, Model generate: LKSCHMQWHG  |The result is True
Input: ['STNSMVFJCB'], Should get: BCJFVMSNTS, Model generate: BCJFVMSNTS  |The result is True
Input: ['ZNULXMCYGD'], Should get: DGYCMXLUNZ, Model generate: DGYCMXLUZN  |The result is False
Input: ['AGMUZWIUZA'], Should get: AZUIWZUMGA, Model generate: AZUWIZUMGA  |The result is False
Input: ['DIKJDSSGVR'], Should get: RVGSSDJKID, Model generate: RVGSSDJKID  |The result is True
Input: ['ENNPEHSLPL'], Should get: LPLSHEPNNE, Model generate: LPLSHEPNSN  |The result is False
Input: ['VQFLKQYFWT'], Should get: TWFYQKLFQV, Model generate: TWFYQKLFQV  |The result is True
Input: ['EGAJLHRLZH'], Should get: HZLRHLJAGE, Model generate: HZLRHLJGAE  |The result is False
Input: ['JHHILOHGRU'], Should get: URGHOLIHHJ, Model generate: URGHOLIHHJ  |The result is True
Input: ['OJAYOEUIHN'], Should get: NHIUEOYAJO, Model generate: NHIUEOYAJO  |The result is True
Input: ['UCQFANUXKO'], Should get: OKXUNAFQCU, Model generate: OKXUNAFQCU  |The result is True
Input: ['OJMINBGNVZ'], Should get: ZVNGBNIMJO, Model generate: ZVNGBNIMJO  |The result is True
Input: ['HKSUMPCWFP'], Should get: PFWCPMUSKH, Model generate: PFWCPMUSKH  |The result is True
Input: ['MZPTJMFHLZ'], Should get: ZLHFMJTPZM, Model generate: ZHLFMJPTZG  |The result is False
Input: ['YEGFOBGNVH'], Should get: HVNGBOFGEY, Model generate: HVNGBOFGEY  |The result is True
Input: ['VZKAPJEBBV'], Should get: VBBEJPAKZV, Model generate: VBBEJPAKZV  |The result is True
Input: ['ZHIMEEBYPG'], Should get: GPYBEEMIHZ, Model generate: GPYEBMEIHZ  |The result is False
Input: ['USGDZQYZIK'], Should get: KIZYQZDGSU, Model generate: KIZYQZDGSU  |The result is True
Input: ['KCHEVDSYLC'], Should get: CLYSDVEHCK, Model generate: CLYSDVEHKC  |The result is False
Input: ['FEWCDKIFVC'], Should get: CVFIKDCWEF, Model generate: CVFIKDCFWE  |The result is False
Input: ['NPQVKFLDIE'], Should get: EIDLFKVQPN, Model generate: EIDLFKVQPN  |The result is True
Input: ['SICKFRCPYJ'], Should get: JYPCRFKCIS, Model generate: JYPCFRKICR  |The result is False
Input: ['PEZIQCZIXN'], Should get: NXIZCQIZEP, Model generate: NXIZCQIZEP  |The result is True
Input: ['WNGCKAOWBA'], Should get: ABWOAKCGNW, Model generate: ABWOAKGCNW  |The result is False
Input: ['SMKVKTWINT'], Should get: TNIWTKVKMS, Model generate: TNIWTKVKMS  |The result is True
Input: ['YAVKXMYPCJ'], Should get: JCPYMXKVAY, Model generate: JCPYMXKVAY  |The result is True
Input: ['JILOPWPBRV'], Should get: VRBPWPOLIJ, Model generate: VRBPWPOILJ  |The result is False
Input: ['ZXNNYVRHMY'], Should get: YMHRVYNNXZ, Model generate: YMHRVYNNXZ  |The result is True
Input: ['PQGKPIGKCK'], Should get: KCKGIPKGQP, Model generate: KCKGIPKGQP  |The result is True
Input: ['MKWXQLVFKK'], Should get: KKFVLQXWKM, Model generate: KKFVLQXWKM  |The result is True
Input: ['KJEIQGNLKW'], Should get: WKLNGQIEJK, Model generate: WKLNGQIEJK  |The result is True
Input: ['HDHDQOLWUU'], Should get: UUWLOQDHDH, Model generate: UUWLOQDHDH  |The result is True
Input: ['SRJDSYPVPZ'], Should get: ZPVPYSDJRS, Model generate: ZPVPYSDJRX  |The result is False
Input: ['GXVEVTBWYI'], Should get: IYWBTVEVXG, Model generate: IYWBTVEVXG  |The result is True
Input: ['MPFSKRLRLO'], Should get: OLRLRKSFPM, Model generate: OLRLRKSFPM  |The result is True
Input: ['QPHTQFLUKQ'], Should get: QKULFQTHPQ, Model generate: QKULFQTHPQ  |The result is True
Input: ['JRXGPKZFTV'], Should get: VTFZKPGXRJ, Model generate: VTFZKPGXRJ  |The result is True
Input: ['CYEBEYATMX'], Should get: XMTAYEBEYC, Model generate: XMTAYEBYGE  |The result is False
Input: ['ICPMDGOLVG'], Should get: GVLOGDMPCI, Model generate: GVLGODPMIC  |The result is False
Input: ['WOTNQTUQRB'], Should get: BRQUTQNTOW, Model generate: BRQUTQNTOW  |The result is True
Input: ['KNWFJMJOFG'], Should get: GFOJMJFWNK, Model generate: GFOJMJFWNF  |The result is False
Input: ['FNGCRAOMMV'], Should get: VMMOARCGNF, Model generate: VMMOARCGNF  |The result is True
Input: ['GIONBNEBZG'], Should get: GZBENBNOIG, Model generate: GZBENBONIG  |The result is False
Input: ['NXLAHBXWCB'], Should get: BCWXBHALXN, Model generate: BCWXBHALXN  |The result is True
Input: ['SUEIODATAN'], Should get: NATADOIEUS, Model generate: NATADOIEUS  |The result is True
Input: ['GOFWZKDLRO'], Should get: ORLDKZWFOG, Model generate: ORLDKZWFOG  |The result is True
Input: ['ZVQHUNWEXX'], Should get: XXEWNUHQVZ, Model generate: XXEWNUHQVZ  |The result is True
Input: ['RAEMGWHQXD'], Should get: DXQHWGMEAR, Model generate: DXQHWGMEAR  |The result is True
Input: ['IVJBLVORNM'], Should get: MNROVLBJVI, Model generate: MNROVLBJIV  |The result is False
Input: ['LOJPJDYSYM'], Should get: MYSYDJPJOL, Model generate: MYSYDJPJOH  |The result is False
Input: ['CTADHKYGSW'], Should get: WSGYKHDATC, Model generate: WSGKYHDATC  |The result is False
Input: ['NNODEBNMTA'], Should get: ATMNBEDONN, Model generate: ATMNBEDONN  |The result is True
Input: ['TZIRKDIJLS'], Should get: SLJIDKRIZT, Model generate: SLJIDKRIZT  |The result is True
Input: ['HICXLDDYQZ'], Should get: ZQYDDLXCIH, Model generate: ZQDYDLXICH  |The result is False
Input: ['ULFYEWNMFZ'], Should get: ZFMNWEYFLU, Model generate: ZFMNWEYFLU  |The result is True
Input: ['YPLUZYHVDC'], Should get: CDVHYZULPY, Model generate: CDVHYZULPY  |The result is True
Input: ['HWTEASSOGS'], Should get: SGOSSAETWH, Model generate: SGOSSEAWTH  |The result is False
Input: ['MRBKIQRTUW'], Should get: WUTRQIKBRM, Model generate: WUTRQIKBRM  |The result is True
Input: ['RAJRHFJXBJ'], Should get: JBXJFHRJAR, Model generate: JBXJFHRAJR  |The result is False
Input: ['HRNRONHRUS'], Should get: SURHNORNRH, Model generate: SURHNORNRH  |The result is True
Input: ['BNGPVSIDHF'], Should get: FHDISVPGNB, Model generate: FHDISVPGNB  |The result is True
Input: ['WRARKFZDMA'], Should get: AMDZFKRARW, Model generate: AMDFZKRARW  |The result is False
Input: ['LVUJKNIVKW'], Should get: WKVINKJUVL, Model generate: WKVINKJUVL  |The result is True
Input: ['NYHDNGMNFM'], Should get: MFNMGNDHYN, Model generate: MFNMGNDHYN  |The result is True
Input: ['BZOJQKTAWE'], Should get: EWATKQJOZB, Model generate: EWATKQJOBZ  |The result is False
Input: ['FSMFQZVGXM'], Should get: MXGVZQFMSF, Model generate: MXGVZQFMFS  |The result is False
Input: ['JMSXYKJPVR'], Should get: RVPJKYXSMJ, Model generate: RVPJKYXSMJ  |The result is True
Input: ['WMSQPRRYHR'], Should get: RHYRRPQSMW, Model generate: RHRYPRQSHU  |The result is False
Input: ['LPPTJQZKSD'], Should get: DSKZQJTPPL, Model generate: DSKZQJTPPI  |The result is False
Input: ['DWVDIHWMSA'], Should get: ASMWHIDVWD, Model generate: ASMWHIDVWD  |The result is True
Input: ['HDDFIHKKAE'], Should get: EAKKHIFDDH, Model generate: EAKKHIFDDH  |The result is True
Input: ['BVNBJHPPNK'], Should get: KNPPHJBNVB, Model generate: KNPPHJBNVB  |The result is True
Input: ['QBNGZGWXET'], Should get: TEXWGZGNBQ, Model generate: TEXWGZGNBQ  |The result is True
Input: ['WAZXMHKWRY'], Should get: YRWKHMXZAW, Model generate: YRWKHMXZAW  |The result is True
"""
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import string
import time

SEQ_END_SYMBOL = '<EOS>'
UPPER_CHARS_NUMBER = 26  # 大写字母个数
EMBEDDING_LAYER_DIM = 64
GRU_HIDDEN_DIM = 100
BATCH_SIZE = 64
SEQ_LENGTH = 10  # 序列长度

# 制造数据集


def char2index(char):
    if char == SEQ_END_SYMBOL:
        return 0
    else:
        return ord(char) - ord('A') + 1


def index2char(index):
    if index == 0:
        return SEQ_END_SYMBOL
    else:
        return chr(index + ord('A') - 1)


def randomString(stringLength):
    """
    生成stringLength长度的 大写字符串(由A-Z组成)
    """
    letters = string.ascii_uppercase
    return ''.join(random.choice(letters) for i in range(stringLength))


def get_batch(batch_size, length):
    """
    生成训练数据(随机length长度的大写字母序列)
    """
    batch_examples = [randomString(length) for i in range(batch_size)]
    encoder_X = [[char2index(char) for char in list(SampleString)]
                 for SampleString in batch_examples]
    Y = [list(reversed(IndexList)) for IndexList in encoder_X]
    decoder_X = [[char2index(SEQ_END_SYMBOL)] + IndexList[:-1]
                 for IndexList in Y]
    return batch_examples,\
        torch.LongTensor(encoder_X),\
        torch.LongTensor(Y),\
        torch.LongTensor(decoder_X)


class Seq2SeqModel(nn.Module):
    def __init__(self):
        super(Seq2SeqModel, self).__init__()
        self.vocab_size = UPPER_CHARS_NUMBER + 1
        self.embedding_layer = nn.Embedding(
            self.vocab_size, embedding_dim=EMBEDDING_LAYER_DIM)
        #####################################################################################
        # 填空
        # 创建一个输入维度为EMBEDDING_LAYER_DIM，隐藏层维度为GRU_HIDDEN_DIM的单层单向GRU作为encoder
        self.encoder = nn.GRU(
            input_size=EMBEDDING_LAYER_DIM,
            hidden_size=GRU_HIDDEN_DIM,
            num_layers=1,
            batch_first=True
        )
        #####################################################################################
        self.decoder = nn.GRU(
            input_size=EMBEDDING_LAYER_DIM,
            hidden_size=GRU_HIDDEN_DIM,
            num_layers=1,
            batch_first=True
        )
        self.linear = nn.Linear(GRU_HIDDEN_DIM, self.vocab_size)

    def forward(self, encoder_X, decoder_X):

        #######################################################################################
        # encoder_X是encoder的输入序列，为(batch_size,sequence_size)的字符index Tensor
        # decoder_X是decoder的输入序列，为(batch_size,sequence_size)的字符index Tensor
        # 填空
        # 1. 使用编码器对输入的序列进行编码，得到当前的隐藏层状态(hidden_state)
        # print("encoder_X.shape:", encoder_X.shape)
        _, hidden_state = self.encoding(encoder_X)
        # print("encoder_output.shape: ", encoder_output.shape,
        #   "hidden_state.shape: ", hidden_state)
        # 2. 使用encoder得到的隐藏层状态作为decoder的初始隐藏层状态(hidden_state)
        decoder_output, _ = self.decoding(
            decoder_X, hidden_state)
        # 根据decoder每一位的hidden state预测对应的字符可能是哪个
        logit = self.linear(decoder_output).view(-1, self.vocab_size)
        # decoder_output  size: (batch_size,sequence_length,hidden_size)
        #######################################################################################
        return logit

    def encoding(self, encoder_X):
        # (batch_size, sequence_length, embedding_size)
        encoder_embedding = self.embedding_layer(encoder_X)
        encoder_output, hidden_state = self.encoder(encoder_embedding)
        # hidden_state: (整个序列输入后)当前encoder隐藏层状态
        # hidden_state shape: (1, batch_size, hidden_size)
        # encoder_out: 对于输入序列中每一位输入对应的encoder隐藏层状态
        # encoder_out shape： (batch_size, sequence_length, hidden_size)
        return encoder_output, hidden_state

    def decoding(self, decoder_X, hidden_state):
        all_outputs = []
        for i in range(SEQ_LENGTH):
            input_X = decoder_X[:, i].unsqueeze(1)  # (batch_size,1)
            input_embedding = self.embedding_layer(input_X)
            output, hidden_state = self.decoder(input_embedding, hidden_state)
            all_outputs.append(output)
        # (batch_size,sequence_length,hidden_size)
        # print("all_outputs.shape1: ", all_outputs[0].shape)
        all_outputs = torch.cat(all_outputs, dim=1)
        # print("all_outputs.shape2: ", all_outputs.shape)
        return all_outputs, hidden_state

    def get_next_token(self, input_X, hidden_state):
        # 此函数只用于测试(生成序列)
        # 根据hidden state和当前序列的最后一个字符预测下一个字符是啥
        # input_X shape: (batch_size, 1)
        input_embedding = self.embedding_layer(input_X)
        output, new_hidden_state = self.decoder(input_embedding, hidden_state)
        output = self.linear(output.squeeze(1))
        _, output = torch.topk(output, k=1, dim=-1)  # 找到分数最高(概率最大)的字符
        return output.view(-1), new_hidden_state


def train_one_step(model, optimizer, criterion, encoder_X, Y, decoder_X):
    using_time = time.time()
    optimizer.zero_grad()
    logits = model(encoder_X, decoder_X)
    loss = criterion(logits, Y)
    loss_value = loss.item()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    using_time = time.time() - using_time
    return loss_value, using_time


def train(model, optimizer, criterion):
    for step in range(5000):
        batch_samples, encoder_X, Y, decoder_X = get_batch(
            BATCH_SIZE, SEQ_LENGTH)
        Y = Y.view(-1)
        loss, using_time = train_one_step(
            model, optimizer, criterion, encoder_X, Y, decoder_X)
        if (step + 1) % 100 == 0:
            print("step {} loss: {}  -- using time {:3f}".format(step, loss, using_time))


def test(model):
    test_num = 100  # 测试示例个数
    for one in range(test_num):
        # 创建一例测试用的数据
        batch_samples, encoder_X, Y, decoder_X = get_batch(1, SEQ_LENGTH)

        original_sequence = "".join(reversed(batch_samples[0]))  # 应该生成的序列
        generate_sequence = ""  # 实际生成的序列

        # 先用encoder编码
        _, hidden_state = model.encoding(encoder_X)
        current_Char = SEQ_END_SYMBOL
        current_Index = char2index(current_Char)
        # 然后用decoder解码，进行序列生成，当前用decoder生成的字符是下次decoder的输入字符
        with torch.no_grad():
            for i in range(SEQ_LENGTH):
                input_X = torch.LongTensor([[current_Index]])  # .cuda()
                output, hidden_state = model.get_next_token(
                    input_X, hidden_state)
                current_Index = output.cpu().tolist()[0]
                current_Char = index2char(current_Index)
                generate_sequence += current_Char
        print("Input: {}, Should get: {}, Model generate: {}  |The result is {}".format(
            batch_samples, original_sequence, generate_sequence, original_sequence == generate_sequence))


if __name__ == '__main__':
    seq2seq = Seq2SeqModel()
    optimizer = optim.Adam(seq2seq.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    train_using_time = time.time()
    train(seq2seq, optimizer, criterion)
    print("model training all using time {:.3f}".format(
        time.time()-train_using_time))
    test(seq2seq)
