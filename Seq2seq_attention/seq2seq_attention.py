"""
step 4999 loss: 0.3835964798927307
Input: ['TABISGJWVG'], Should get: GVWJGSIBAT, Model generate: GVWJGSIBAT  |The result is True
Input: ['MZXVBMPBIO'], Should get: OIBPMBVXZM, Model generate: OIBPMBPMBP  |The result is False
Input: ['JNVOWTXNGD'], Should get: DGNXTWOVNJ, Model generate: DGNXTWOVNJ  |The result is True
Input: ['CPEMWUOAZF'], Should get: FZAOUWMEPC, Model generate: FZAOUWMEPC  |The result is True
Input: ['RPUCYUEFAW'], Should get: WAFEUYCUPR, Model generate: WAFEUYCUPR  |The result is True
Input: ['QBTCUQSGOO'], Should get: OOGSQUCTBQ, Model generate: OOGSQUCBTC  |The result is False
Input: ['KCCRLVAYUN'], Should get: NUYAVLRCCK, Model generate: YUYAVLRCKC  |The result is False
Input: ['KATKYMWQPY'], Should get: YPQWMYKTAK, Model generate: YPQWMYPQWM  |The result is False
Input: ['ZFIUSHRYGP'], Should get: PGYRHSUIFZ, Model generate: PGYRHSUIFZ  |The result is True
Input: ['PAEXRZYXGM'], Should get: MGXYZRXEAP, Model generate: MGXYZRXEPA  |The result is False
Input: ['OWAZHMVEXG'], Should get: GXEVMHZAWO, Model generate: GXEVMHZAWO  |The result is True
Input: ['BODWUMZWUS'], Should get: SUWZMUWDOB, Model generate: SUWZMUWDOB  |The result is True
Input: ['FYIRWCTXPQ'], Should get: QPXTCWRIYF, Model generate: QPXTCWRIYF  |The result is True
Input: ['JYAWUENPRX'], Should get: XRPNEUWAYJ, Model generate: XRPNEUWOAY  |The result is False
Input: ['ZIUQKTUHFV'], Should get: VFHUTKQUIZ, Model generate: VFHUTKQUIZ  |The result is True
Input: ['OSROCMAGYM'], Should get: MYGAMCORSO, Model generate: MYGAMCOSRO  |The result is False
Input: ['MFHQWZJIOO'], Should get: OOIJZWQHFM, Model generate: OOIJZWQHFM  |The result is True
Input: ['BUOYEXZOIJ'], Should get: JIOZXEYOUB, Model generate: JIOZXEYOUB  |The result is True
Input: ['IMCHRTQKMY'], Should get: YMKQTRHCMI, Model generate: YMQKRTPCHC  |The result is False
Input: ['XZWENNBXDV'], Should get: VDXBNNEWZX, Model generate: VDXBNEWZXX  |The result is False
Input: ['AWFHZKZXPF'], Should get: FPXZKZHFWA, Model generate: FPXZKZHFWA  |The result is True
Input: ['QOEJXBFDRM'], Should get: MRDFBXJEOQ, Model generate: MDRBFBJXEO  |The result is False
Input: ['NTBPGFREDU'], Should get: UDERFGPBTN, Model generate: UDEGRFGPBN  |The result is False
Input: ['DAPLMMBNPF'], Should get: FPNBMMLPAD, Model generate: FPNMBMPLPA  |The result is False
Input: ['XRICMJNHVF'], Should get: FVHNJMCIRX, Model generate: FVHNJMCIRX  |The result is True
Input: ['WSSECIRNAZ'], Should get: ZANRICESSW, Model generate: ZANRICESSW  |The result is True
Input: ['FLLINATNGN'], Should get: NGNTANILLF, Model generate: NGNTANILLF  |The result is True
Input: ['UVZZIUCEIS'], Should get: SIECUIZZVU, Model generate: SIECUIZZVU  |The result is True
Input: ['ATPRQMEZTZ'], Should get: ZTZEMQRPTA, Model generate: ZTZEMQRPTA  |The result is True
Input: ['ZDKOPEDIZL'], Should get: LZIDEPOKDZ, Model generate: LZIDEPOKDZ  |The result is True
Input: ['KXXMETKVFF'], Should get: FFVKTEMXXK, Model generate: FFVKTEMXXK  |The result is True
Input: ['SMAFWRGBSN'], Should get: NSBGRWFAMS, Model generate: NSBGRWFAMS  |The result is True
Input: ['JPFIZAQRPH'], Should get: HPRQAZIFPJ, Model generate: HPRAQZAQFI  |The result is False
Input: ['ZVICMAHLFM'], Should get: MFLHAMCIVZ, Model generate: MFLHACMIVZ  |The result is False
Input: ['LZVBXUECZN'], Should get: NZCEUXBVZL, Model generate: NZECEUXBVZ  |The result is False
Input: ['UYFVTKJDOH'], Should get: HODJKTVFYU, Model generate: HODJKTVFYU  |The result is True
Input: ['QYTZIDKDLS'], Should get: SLDKDIZTYQ, Model generate: SLDKDIZTYQ  |The result is True
Input: ['QUXGQCVQRT'], Should get: TRQVCQGXUQ, Model generate: TRQVCQGXUQ  |The result is True
Input: ['YJOPDMAUYE'], Should get: EYUAMDPOJY, Model generate: EYUAMDPOJY  |The result is True
Input: ['FTIPMSXUNQ'], Should get: QNUXSMPITF, Model generate: QNXMSPMPIT  |The result is False
Input: ['UORLSYJXXY'], Should get: YXXJYSLROU, Model generate: YXXJYSLROU  |The result is True
Input: ['PFVACRRXJL'], Should get: LJXRRCAVFP, Model generate: LJXRCAVFPP  |The result is False
Input: ['GEPMPQIMOP'], Should get: POMIQPMPEG, Model generate: POIMQPPMPQ  |The result is False
Input: ['VVBDJILGUM'], Should get: MUGLIJDBVV, Model generate: MUGLIJDBVV  |The result is True
Input: ['UHQSFFQQLW'], Should get: WLQQFFSQHU, Model generate: WLQFFQQFFS  |The result is False
Input: ['SEQCTEWEEA'], Should get: AEEWETCQES, Model generate: AEEWEEWEEW  |The result is False
Input: ['OYVSGUPJLI'], Should get: ILJPUGSVYO, Model generate: ILJPUGSVYO  |The result is True
Input: ['EULSVNNSFU'], Should get: UFSNNVSLUE, Model generate: UNFSNNVUSL  |The result is False
Input: ['DBAIZTVBWT'], Should get: TWBVTZIABD, Model generate: TWBVTZIABD  |The result is True
Input: ['BZNUZRMFZQ'], Should get: QZFMRZUNZB, Model generate: QZFMRZUNZB  |The result is True
Input: ['WAMCGJAACQ'], Should get: QCAAJGCMAW, Model generate: QCAJGCMAWW  |The result is False
Input: ['ZYTCNNOORJ'], Should get: JROONNCTYZ, Model generate: JRONOYZZJO  |The result is False
Input: ['FBQPBYKDJM'], Should get: MJDKYBPQBF, Model generate: MJDKYBPQBF  |The result is True
Input: ['VFFNJJREVQ'], Should get: QVERJJNFFV, Model generate: QVREVJRJNF  |The result is False
Input: ['YATSASOLIW'], Should get: WILOSASTAY, Model generate: WILSATYSOS  |The result is False
Input: ['OPTCWDASNU'], Should get: UNSADWCTPO, Model generate: UNSADWCTPO  |The result is True
Input: ['DTQJBJBGFZ'], Should get: ZFGBJBJQTD, Model generate: ZFGBJQTDDB  |The result is False
Input: ['NINEKIZMGU'], Should get: UGMZIKENIN, Model generate: UGMZIKENIN  |The result is True
Input: ['FLCYETRTXO'], Should get: OXTRTEYCLF, Model generate: OXTRTEYCLF  |The result is True
Input: ['BYJUMKDFEC'], Should get: CEFDKMUJYB, Model generate: CEFDKMUJYB  |The result is True
Input: ['CMLUFTTCHM'], Should get: MHCTTFULMC, Model generate: MHCTTFULMH  |The result is False
Input: ['OARTRQSBYW'], Should get: WYBSQRTRAO, Model generate: WYBSQRTOAO  |The result is False
Input: ['APGNZWATYN'], Should get: NYTAWZNGPA, Model generate: NYTAWZNGPA  |The result is True
Input: ['CGDYSOPUPV'], Should get: VPUPOSYDGC, Model generate: VPUPOSYDGC  |The result is True
Input: ['DSWTBGJEHM'], Should get: MHEJGBTWSD, Model generate: MHEJGBWTWS  |The result is False
Input: ['PAQLCBBRWI'], Should get: IWRBBCLQAP, Model generate: IWQAPPCLQA  |The result is False
Input: ['AMFJMLGODF'], Should get: FDOGLMJFMA, Model generate: FDOGLMJFMA  |The result is True
Input: ['PTHQDEEIWS'], Should get: SWIEEDQHTP, Model generate: SWIEEDQHTP  |The result is True
Input: ['YKWBUXFDCY'], Should get: YCDFXUBWKY, Model generate: YCDFXUBWKY  |The result is True
Input: ['KJMAXFYSSB'], Should get: BSSYFXAMJK, Model generate: SBSYFXAMJK  |The result is False
Input: ['KVGAXPSELX'], Should get: XLESPXAGVK, Model generate: XLESPXAGVK  |The result is True
Input: ['CNIKXQVDJZ'], Should get: ZJDVQXKINC, Model generate: ZJDVQXKINC  |The result is True
Input: ['XHRMDQJADQ'], Should get: QDAJQDMRHX, Model generate: QDAJQDMRHX  |The result is True
Input: ['GWMQZSZAXE'], Should get: EXAZSZQMWG, Model generate: EXAZSZQMWG  |The result is True
Input: ['XKRNLNNJVT'], Should get: TVJNNLNRKX, Model generate: TVJNNLNRKX  |The result is True
Input: ['AFTMIXCTYK'], Should get: KYTCXIMTFA, Model generate: KYTCXIMTFA  |The result is True
Input: ['BZFESATVWK'], Should get: KWVTASEFZB, Model generate: KWVTASEFZB  |The result is True
Input: ['HTCZUDRSLF'], Should get: FLSRDUZCTH, Model generate: FLSRSDUZCT  |The result is False
Input: ['ODDANVLMOY'], Should get: YOMLVNADDO, Model generate: YOMLVNADDO  |The result is True
Input: ['CVVNZBMNEV'], Should get: VENMBZNVVC, Model generate: VENMBPBZMB  |The result is False
Input: ['WJXAFRIMHW'], Should get: WHMIRFAXJW, Model generate: WHMIRFAXJW  |The result is True
Input: ['HWIGCALQJK'], Should get: KJQLACGIWH, Model generate: KJQLAGCGIW  |The result is False
Input: ['HJHFDCIINR'], Should get: RNIICDFHJH, Model generate: RNIICDFHJH  |The result is True
Input: ['RWGCUZOXBC'], Should get: CBXOZUCGWR, Model generate: CBXOZUCGWR  |The result is True
Input: ['YIVSVWIVWN'], Should get: NWVIWVSVIY, Model generate: NWIVWIVWVS  |The result is False
Input: ['KEDHOFQCZA'], Should get: AZCQFOHDEK, Model generate: AZCQFOHDEK  |The result is True
Input: ['NCKHTVSOMT'], Should get: TMOSVTHKCN, Model generate: TMSOSVTHKC  |The result is False
Input: ['QYBYICLZJS'], Should get: SJZLCIYBYQ, Model generate: SJZLCIYBYQ  |The result is True
Input: ['TMNMUVJZED'], Should get: DEZJVUMNMT, Model generate: DEZJVUMNMN  |The result is False
Input: ['ACHYZJEVPD'], Should get: DPVEJZYHCA, Model generate: DPVEJZYHCA  |The result is True
Input: ['FJADRCLVCM'], Should get: MCVLCRDAJF, Model generate: MCVLCRDAJF  |The result is True
Input: ['JGBDDBRSZQ'], Should get: QZSRBDDBGJ, Model generate: QZSRBDDGJJ  |The result is False
Input: ['BLQJGPFNSB'], Should get: BSNFPGJQLB, Model generate: SBSNFPGJQL  |The result is False
Input: ['GGJIVSXDXP'], Should get: PXDXSVIJGG, Model generate: PXDXSVIJGG  |The result is True
Input: ['EGFNQOCKAX'], Should get: XAKCOQNFGE, Model generate: XAKCOQNFGE  |The result is True
Input: ['PLJBTVFNHC'], Should get: CHNFVTBJLP, Model generate: CHNFVBTCHN  |The result is False
Input: ['IYDHXJGUOE'], Should get: EOUGJXHDYI, Model generate: EOUGJXHDYI  |The result is True
Input: ['JHRHJCEOTN'], Should get: NTOECJHRHJ, Model generate: NTOECJHRHJ  |The result is True
Input: ['BBNCTZSTBL'], Should get: LBTSZTCNBB, Model generate: LBTSZTCNBB  |The result is True
Input: ['OVFBFBFOYP'], Should get: PYOFBFBFVO, Model generate: PYOFBFBFBF  |The result is False
"""
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import string
import time

SEQ_END_SYMBOL = '<EOS>'
UPPER_CHARS_NUMBER = 26  # 大写字母个数
EMBEDDING_LAYER_DIM = 64
GRU_HIDDEN_DIM = 100
BATCH_SIZE = 64
SEQ_LENGTH = 10  # 序列长度

# 制造数据集


def char2index(char):
    if char == SEQ_END_SYMBOL:
        return 0
    else:
        return ord(char) - ord('A') + 1


def index2char(index):
    if index == 0:
        return SEQ_END_SYMBOL
    else:
        return chr(index + ord('A') - 1)


def randomString(stringLength):
    """
    生成stringLength长度的 大写字符串(由A-Z组成)
    """
    letters = string.ascii_uppercase
    return ''.join(random.choice(letters) for i in range(stringLength))


def get_batch(batch_size, length):
    """
    生成训练数据(随机length长度的大写字母序列)
    """
    batch_examples = [randomString(length) for i in range(batch_size)]
    encoder_X = [[char2index(char) for char in list(SampleString)]
                 for SampleString in batch_examples]
    Y = [list(reversed(IndexList)) for IndexList in encoder_X]
    decoder_X = [[char2index(SEQ_END_SYMBOL)] + IndexList[:-1]
                 for IndexList in Y]
    return batch_examples,\
        torch.LongTensor(encoder_X),\
        torch.LongTensor(Y),\
        torch.LongTensor(decoder_X)


class Seq2SeqModel(nn.Module):
    def __init__(self):
        # 这个seq2seq模型是添加了attention机制的升级版
        # 其中attention权重的计算采取双线性模型
        # 其中decoder的输入是当前时刻t的输入字符的embedding与
        #  根据上一时刻t-1的hidden state和encoder hidden state计算得到的聚合信息向量拼接作为输入
        super(Seq2SeqModel, self).__init__()
        self.vocab_size = UPPER_CHARS_NUMBER + 1
        self.embedding_layer = nn.Embedding(
            self.vocab_size, embedding_dim=EMBEDDING_LAYER_DIM)
        self.encoder = nn.GRU(
            input_size=EMBEDDING_LAYER_DIM,
            hidden_size=GRU_HIDDEN_DIM,
            num_layers=1,
            batch_first=True
        )
        self.decoder = nn.GRU(
            # 将根据attention weight计算得到的 context vector和当前输入字符的embedding拼接作为decoder输入
            input_size=EMBEDDING_LAYER_DIM + GRU_HIDDEN_DIM,
            hidden_size=GRU_HIDDEN_DIM,
            num_layers=1,
            batch_first=True
        )
        self.linear = nn.Linear(GRU_HIDDEN_DIM, self.vocab_size)
        self.attention_W = nn.Linear(
            GRU_HIDDEN_DIM, GRU_HIDDEN_DIM, bias=False)  # 双线性模型attention中的W

    def forward(self, encoder_X, decoder_X):
        encoder_output, hidden_state = self.encoding(encoder_X)
        decoder_output, decoder_hidden = self.decoding(
            encoder_output, decoder_X, hidden_state)
        # decoder output_size: (batch_size,sequence_length,hidden_size)
        logit = self.linear(decoder_output).view(-1, self.vocab_size)
        return logit

    def encoding(self, encoder_X):
        # (batch_size, sequence_length, embedding_size)
        encoder_embedding = self.embedding_layer(encoder_X)
        encoder_output, hidden_state = self.encoder(encoder_embedding)
        # hidden_state: (整个序列输入后)当前encoder隐藏层状态
        # hidden_state shape: (1, batch_size, hidden_size)
        # encoder_out: 对于输入序列中每一位输入对应的encoder隐藏层状态
        # encoder_out shape： (batch_size, sequence_length, hidden_size)
        return encoder_output, hidden_state

    def get_context_vector(self, encoder_out, hidden_state):
        # 用于得到得到聚合信息向量
        # 先计算attention分数后，再根据attention分数得到聚合信息向量(sum(a_1 * encoder_hidden_state_1 + a_2 * encoder_hidden_state_2 + ...))
        #encoder_out: (batch_size, sequence_length, hidden_size)
        #hidden_state: (1, batch_size, hidden_size)
        weight = torch.bmm(self.attention_W(encoder_out), hidden_state.permute(
            1, 2, 0))  # ( batch_size, sequence_length ,1 )
        ###################################################################
        # 填空 对计算出的attention分数进行softmax归一化(要注意对哪一维度的值进行归一化)
        weight = F.softmax(weight, 1)  # (batch_size, sequence_length,1)
        ###################################################################
        # (batch_size, 1, hidden_size)
        context_vectors = torch.sum(weight * encoder_out, dim=1, keepdim=True)
        return context_vectors

    def decoding(self, encoder_out, decoder_X, hidden_state):
        all_outputs = []
        for i in range(SEQ_LENGTH):
            input_X = decoder_X[:, i].unsqueeze(1)  # (batch_size,1)
            # 字符的embedding input_embedding:(batch_size,1, EMBEDDING_LAYER_DIM)
            input_embedding = self.embedding_layer(input_X)
            #####################################################################
            # 填空
            # 1. 得到聚合信息向量(用attention机制计算得到的那个)
            # attention_output :(batch_size, 1, GRU_HIDDEN_DIM)
            attention_output = self.get_context_vector(
                encoder_out, hidden_state)
            # 2. 将字符的embedding与聚合信息向量进行拼接，以作为decoder的输入
            input_embedding = torch.cat([input_embedding, attention_output], 2)
            #####################################################################
            output, hidden_state = self.decoder(input_embedding, hidden_state)
            all_outputs.append(output)
        # (batch_size,sequence_length,hidden_size)
        all_outputs = torch.cat(all_outputs, dim=1)
        return all_outputs, hidden_state

    def get_next_token(self, encoder_out, input_X, hidden_state):
        # 用于预测，根据encoder的状态、当前时刻的字符输入以及上一时刻得到的hidden state预测下一个字符是啥
        # input_X shape: (batch_size, 1)
        # encoder_out shape: (batch_size, sequence_length, hidden_size)
        # hidden_state shape: (1, batch_size,hidden_size)
        #####################################################################################
        # 填空
        # 可以参考一下seq2seq的get next token函数哦
        input_embedding = self.embedding_layer(input_X)
        # input_embedding shape: (batch_size, 1, EMBEDDING_LAYER_DIM)
        attention_input = self.get_context_vector(encoder_out, hidden_state)
        # attention_output :(batch_size, 1, GRU_HIDDEN_DIM)
        input_embedding = torch.cat([input_embedding, attention_input], 2)
        # input_embedding shape: (batch_size, 1, EMBEDDING_LAYER_DIM+GRU_HIDDEN_DIM)
        output, new_hidden_state = self.decoder(input_embedding, hidden_state)
        # output: (batch_size, 1, hidden_size)
        # new_hidden_state (1,batch_size,hidden_size)
        output = self.linear(output.squeeze(1))
        # output: (batch_size, vocab_size)
        _, output = torch.topk(output, k=1, dim=-1)  # 找到分数最高(概率最大)的字符
        #####################################################################################
        # output size: (batch_size)  (对应根据decoder当前隐藏层状态，通过线性层分类得到的，模型认为的最有可能的输出字符的对应index)
        # new_hidden_state (1,batch_size,hidden_size) (当前隐藏层状态)
        # print(output.view(-1).shape)
        return output.view(-1), new_hidden_state


def train_one_step(model, optimizer, criterion, encoder_X, Y, decoder_X):
    using_time = time.time()
    optimizer.zero_grad()
    logits = model(encoder_X, decoder_X)
    loss = criterion(logits, Y)
    loss_value = loss.item()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    using_time = time.time() - using_time
    return loss_value, using_time


def train(model, optimizer, criterion):
    for step in range(5000):
        batch_samples, encoder_X, Y, decoder_X = get_batch(
            BATCH_SIZE, SEQ_LENGTH)
        Y = Y.view(-1)
        loss, using_time = train_one_step(
            model, optimizer, criterion, encoder_X, Y, decoder_X)
        if (step + 1) % 100 == 0:
            print("step {} loss: {}  -- using time {:3f}".format(step, loss, using_time))


def test(model):
    test_num = 100  # 测试示例个数
    for one in range(test_num):
        # 创建一例测试用的数据
        batch_samples, encoder_X, Y, decoder_X = get_batch(1, SEQ_LENGTH)

        original_sequence = "".join(reversed(batch_samples[0]))  # 应该生成的序列
        generate_sequence = ""  # 实际生成的序列

        # 先用encoder编码
        encoder_out, hidden_state = model.encoding(encoder_X)
        current_Char = SEQ_END_SYMBOL
        current_Index = char2index(current_Char)
        # 然后用decoder解码，进行序列生成，当前用decoder生成的字符是下次decoder的输入字符
        with torch.no_grad():
            for i in range(SEQ_LENGTH):
                input_X = torch.LongTensor([[current_Index]])
                output, hidden_state = model.get_next_token(
                    encoder_out, input_X, hidden_state)
                current_Index = output.tolist()[0]
                # print(current_Index)
                current_Char = index2char(current_Index)
                generate_sequence += current_Char
        print("Input: {}, Should get: {}, Model generate: {}  |The result is {}".format(
            batch_samples, original_sequence, generate_sequence, original_sequence == generate_sequence))


if __name__ == '__main__':
    seq2seq = Seq2SeqModel()
    optimizer = optim.Adam(seq2seq.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    train_using_time = time.time()
    train(seq2seq, optimizer, criterion)
    print("model training all using time {:.3f}".format(
        time.time()-train_using_time))
    test(seq2seq)
